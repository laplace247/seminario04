

El Procesamiento de Lenguaje Natural (NLP) es una rama de la
inteligencia artificial que permite que las computadoras entiendan,
interpreten y respondan al lenguaje humano, como el español o el inglés.
El objetivo del NLP es enseñar a las máquinas a comprender las palabras
y frases que usamos para que puedan interactuar con nosotros. Ejemplo
simple: Piensa en un asistente virtual como Siri o Google Assistant.
Ellos usan NLP para entender lo que dices y responderte, ya sea para
decirte el clima o para enviar un mensaje de texto.

    #Tokenizacion (separar los elementos de la oracion)
    import nltk
    nltk.download('punkt')
    from nltk.tokenize import word_tokenize

    texto = "Hola, ¿cómo estás? Espero que todo te vaya bien brother"
    tokens = word_tokenize(texto)
    print(tokens)

    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Unzipping tokenizers/punkt.zip.

    ['Hola', ',', '¿cómo', 'estás', '?', 'Espero', 'que', 'todo', 'te', 'vaya', 'bien', 'brother']

Explicacion: word_tokenize, lo que hace es separar las palabras de forma
individual (tokens), teniendo como respuesta una lista de palabras

    #Eliminar palabras conectoras y vacias (stopwords)

    from nltk.corpus import stopwords
    nltk.download('stopwords')

    word = ['esta','es','una','clase','de','programacion','de','nivel','intermedio']
    stop_words = set(stopwords.words('spanish'))
    filtered_words = [word for word in word if word not in stop_words]
    print(filtered_words)

    ['clase', 'programacion', 'nivel', 'intermedio']

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!

    #Lematizacion
    #Reduce una palabra a su forma base
    from nltk.stem import WordNetLemmatizer
    nltk.download('wordnet')

    lemmatizer = WordNetLemmatizer()
    word = 'running'
    print(lemmatizer.lemmatize(word, pos='v'))

    [nltk_data] Downloading package wordnet to /root/nltk_data...

    run

    #Conteo de palabras (redundancia)
    from collections import Counter
    #Counter, contabiliza las veces que se repite una palabra o elemento en una lista
    texto = "Hola, estoy aprendiendo el procesamiento del lenguaje natural y estoy aburrido :)"
    tokens = word_tokenize(texto.lower())
    word_count = Counter(tokens)
    print(word_count)

    Counter({'estoy': 2, 'hola': 1, ',': 1, 'aprendiendo': 1, 'el': 1, 'procesamiento': 1, 'del': 1, 'lenguaje': 1, 'natural': 1, 'y': 1, 'aburrido': 1, ':': 1, ')': 1})

    #Etiquetado de partes de la oracion - pos tagging
    #Etiquetas gramaticales (verbos, sustantivos, etc)
    nltk.download('averaged_perceptron_tagger')

    texto = 'alguien se esta quedando en la calle'
    tokens = word_tokenize(texto)
    pos_tags = nltk.pos_tag(tokens)
    print(pos_tags)

    '''
    NNP = nombre propio
    VBZ = verbo
    JJ = adjetivo
    NN = sustantivo
    , = coma
    '''

    [('alguien', 'NN'), ('se', 'NN'), ('esta', 'NN'), ('quedando', 'NN'), ('en', 'IN'), ('la', 'FW'), ('calle', 'NN')]

    [nltk_data] Downloading package averaged_perceptron_tagger to
    [nltk_data]     /root/nltk_data...
    [nltk_data]   Package averaged_perceptron_tagger is already up-to-
    [nltk_data]       date!

    #Traduccion basica con googletrans
    !pip install googletrans==4.0.0-rc1
    from googletrans import Translator

    translator = Translator()
    texto = "El sol se ocultaba en el horizonte, tiñendo el cielo de tonos anaranjados y púrpuras. Las olas del mar rompían suavemente en la orilla, creando una melodía tranquilizadora que acompañaba el canto de las gaviotas. En la playa, una familia reía mientras construía castillos de arena, inmortalizando esos momentos de felicidad. El aroma a sal y a brisa marina llenaba el aire, recordando la belleza de lo simple y lo efímero. Era un instante perfecto, donde el tiempo parecía detenerse, permitiendo que la naturaleza y la vida se fundieran en un abrazo de serenidad."
    traduccion = translator.translate(texto, src='es', dest='en')
    print(traduccion.text)

    Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)
    Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)
    Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)
    Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.9.1)
    Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)
    Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)
    Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)
    Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)
    Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)
    Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)
    Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)
    Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)
    Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)
    The sun was hidden on the horizon, staining the sky of orange and purple tones.The waves of the sea broke gently on the shore, creating a reassuring melody that accompanied the song of the seagulls.On the beach, a family laughed while building sand castles, immortalizing those moments of happiness.The aroma of salt and sea breeze filled the air, remembering the beauty of the simple and the ephemeral.It was a perfect moment, where time seemed to stop, allowing nature and life to melt into a hug of serenity.

    # Instalar la librería TextBlob si no está instalada
    !pip install textblob

    # Importar las librerías necesarias
    from textblob import TextBlob
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer

    #descargas de recursos
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('wordnet')

    #Inicializarlo lematizacion y stopwords
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('spanish'))


    # Lista de reseñas de productos
    reseñas = [
        "Este producto es malo",
        "No vale la pena, es muy caro para lo que ofrece.",
        "Satisfecho con la compra, buena calidad.",
        "El peor servicio al cliente que he recibido.",
        "Un artículo increíble, funciona de maravilla.",
        "Muy mala experiencia, llegó dañado.",
        "Lo recomiendo, muy buen producto.",
        "Nunca volvería a comprar esto, pésima calidad."
    ]


    #Diccionario de palabras clave y sus puntuaciones
    puntuaciones = {
        'bueno':1,
        'satisfecho':1,
        'recomiendo':1,
        'increíble':1,
        'muy bueno':1,
        'excelente':1,
        'buena':1,
        'muy buena':1,
        'malo':-1,
        'peor':-1,
        'pésimo':-1,
        'pésima':-1,
        'muy malo':-1,
        'muy pésimo':-1,
        'no vale la pena':-1,
        'caro':-1,
        'muy caro':-1,
        'muy barato':1,
        'barato':1,
        'dañado':-1,
        'mala':-1
    }

    # Función para realizar análisis de sentimientos con TextBlob
    def analizar_sentimiento(texto):
        #tokenizar y lematizar el texto
        palabras = word_tokenize(texto.lower())
        palabras_limpias = [lemmatizer.lemmatize(palabra) for palabra in palabras if palabra.isalnum() and palabra not in stop_words]

        #Analizar el sentimiento con el textblob
        blob = TextBlob(' '.join(palabras_limpias))
        polaridad = blob.sentiment.polarity  # Polaridad entre -1 (negativo) y 1 (positivo)

        #Ajustar la polaridad segun la palabra clave
        score = polaridad
        for palabra in palabras_limpias:
            if palabra in puntuaciones:
                score += puntuaciones[palabra]
        #clasificacion del sentimiento
        if score > 0.2:
            return 'Positivo'
        elif score < -0.2:
            return 'Negativo'
        else:
            return 'Neutro'

    # Aplicar análisis de sentimientos a cada reseña
    for reseña in reseñas:
        sentimiento = analizar_sentimiento(reseña)
        print(f"Reseña: {reseña}")
        print(f"Sentimiento: {sentimiento}\n")

    Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)
    Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)
    Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)
    Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)
    Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)
    Reseña: Este producto es malo
    Sentimiento: Negativo

    Reseña: No vale la pena, es muy caro para lo que ofrece.
    Sentimiento: Negativo

    Reseña: Satisfecho con la compra, buena calidad.
    Sentimiento: Positivo

    Reseña: El peor servicio al cliente que he recibido.
    Sentimiento: Negativo

    Reseña: Un artículo increíble, funciona de maravilla.
    Sentimiento: Positivo

    Reseña: Muy mala experiencia, llegó dañado.
    Sentimiento: Negativo

    Reseña: Lo recomiendo, muy buen producto.
    Sentimiento: Positivo

    Reseña: Nunca volvería a comprar esto, pésima calidad.
    Sentimiento: Negativo

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!
    [nltk_data] Downloading package wordnet to /root/nltk_data...
    [nltk_data]   Package wordnet is already up-to-date!

    #Ejercicio 01

    #Importar librerias necesarias
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from collections import Counter

    #Descargar recursos nltk
    nltk.download('punkt')
    nltk.download('stopwords')

    #Generar un lista de comentarios en español
    comentarios = [
        "Me encanto el producto, lo recomiendo para todos!",
        "El servicio fue muy malo, no volvere"
        "La calidad es excelente, estoy muy satisfecho con la compra"
        "Es un articulo caro y no vale la pena"
        "El producto llego dañado, estoy muy decepcionado"
        "No me gusto le producto, no lo recomiendo"
        "Excelente servicio lo volveria a comprar nuevamente"
        "El producto es de buena calidad, lo recomiendo"
    ]
    #Tokenizacion de los Comentarios
    tokens_comentarios = [word_tokenize(comentario.lower()) for comentario in comentarios]
    print("Tokens de los Comentarios")
    print(tokens_comentarios)

    #Eliminar los Stopwords
    stop_words = set(stopwords.words('spanish'))
    comentarios_filtrados = [[word for word in tokens if word not in stop_words] for tokens in tokens_comentarios]
    print("\nComentarios Filtrados")
    print(comentarios_filtrados)

    #Contar las frecuencias de las palabras
    todas_palabras = [palabra for comentario in comentarios_filtrados for palabra in comentario]
    frecuencia_palabras = Counter(todas_palabras)
    print("\nFrecuencia de las palabras")
    print(frecuencia_palabras)

    #Definir listas de palabras positivas y negativas
    palabras_positivas = ['encanto','recomiendo','excelente','mejor','bueno','satisfecho','maravilloso','buena','fantastico','magnifico','buen']
    palabras_negativas = ['dañado','malo','caro','peor','desagrado','decepcionado']

    #Contar las palabras positivas y negativas en los comentarios

    conteo_positivo = sum([frecuencia_palabras[word] for word in palabras_positivas])
    conteo_negativo = sum([frecuencia_palabras[word] for word in palabras_negativas])

    print("\nAnalisis de Sentimientos")
    print(f"Comentarios positivos: {conteo_positivo}")
    print(f"Comentarios negativos: {conteo_negativo}")

    #Determinar en sentimiento general del comentario
    print('/nSentimientos en General')
    if conteo_positivo > conteo_negativo:
        print("El sentimiento es positivo")
    elif conteo_negativo > conteo_positivo:
        print("El sentimiento es negativo")
    else:
        print("El sentimiento es neutro")

    Tokens de los Comentarios
    [['me', 'encanto', 'el', 'producto', ',', 'lo', 'recomiendo', 'para', 'todos', '!'], ['el', 'servicio', 'fue', 'muy', 'malo', ',', 'no', 'volverela', 'calidad', 'es', 'excelente', ',', 'estoy', 'muy', 'satisfecho', 'con', 'la', 'compraes', 'un', 'articulo', 'caro', 'y', 'no', 'vale', 'la', 'penael', 'producto', 'llego', 'dañado', ',', 'estoy', 'muy', 'decepcionadono', 'me', 'gusto', 'le', 'producto', ',', 'no', 'lo', 'recomiendoexcelente', 'servicio', 'lo', 'volveria', 'a', 'comprar', 'nuevamenteel', 'producto', 'es', 'de', 'buena', 'calidad', ',', 'lo', 'recomiendo']]

    Comentarios Filtrados
    [['encanto', 'producto', ',', 'recomiendo', '!'], ['servicio', 'malo', ',', 'volverela', 'calidad', 'excelente', ',', 'satisfecho', 'compraes', 'articulo', 'caro', 'vale', 'penael', 'producto', 'llego', 'dañado', ',', 'decepcionadono', 'gusto', 'producto', ',', 'recomiendoexcelente', 'servicio', 'volveria', 'comprar', 'nuevamenteel', 'producto', 'buena', 'calidad', ',', 'recomiendo']]

    Frecuencia de las palabras
    Counter({',': 6, 'producto': 4, 'recomiendo': 2, 'servicio': 2, 'calidad': 2, 'encanto': 1, '!': 1, 'malo': 1, 'volverela': 1, 'excelente': 1, 'satisfecho': 1, 'compraes': 1, 'articulo': 1, 'caro': 1, 'vale': 1, 'penael': 1, 'llego': 1, 'dañado': 1, 'decepcionadono': 1, 'gusto': 1, 'recomiendoexcelente': 1, 'volveria': 1, 'comprar': 1, 'nuevamenteel': 1, 'buena': 1})

    Analisis de Sentimientos
    Comentarios positivos: 5
    Comentarios negativos: 3
    El sentimiento es positivo

    [nltk_data] Downloading package punkt to /root/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!
    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!

LIBRERIAS SCIPY & NLTK SCIPY: Librerias para el desarrollo de funciones
matematicas avanzadas NLTK: Procesamiento de lenguaje natural NLP

    #Resolver ecuaciones lineales
    #Algebra Lineal
    from scipy import linalg
    import numpy as np

    #Definir la matriz de coeficientes y el vector de términos independientes
    X = np.array([[15,8],[12,6]])
    y = np.array([2,3])

    respuesta = linalg.solve(X,y)
    print(respuesta)

    [ 2.  -3.5]

    #Integracion Numerica
    #Es para calcular el area bajo una curva, cuando las formulas son complejas

    from scipy.integrate import quad

    resultado, _ = quad(lambda x: x**2, 0, 1)
    print(resultado)

    #Integracion numerica de la funcion x elevado a 2 entre los limites 0 y 1

    0.33333333333333337

    #Optimizacion con SCIPY
    #Encontrar el valor de minimo o max de una funcion
    from scipy.optimize import minimize

    def objective(x):
      return x**2 + 5*x + 8

    result = minimize(objective, 0)
    print(result.x)

    [-2.50000002]

    #Tokenizacion de oraciones
    from nltk.tokenize import word_tokenize

    texto = "Holaa amigo estoy muy contento de que vengas a mi casa amiguito"
    tokens = word_tokenize(texto)
    print(tokens)

    ['Holaa', 'amigo', 'estoy', 'muy', 'contento', 'de', 'que', 'vengas', 'a', 'mi', 'casa', 'amiguito']
