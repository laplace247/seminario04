{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncUUrijQ7S-I"
      },
      "source": [
        "El Procesamiento de Lenguaje Natural (NLP) es una rama de la inteligencia artificial que permite que las computadoras entiendan, interpreten y respondan al lenguaje humano, como el español o el inglés. El objetivo del NLP es enseñar a las máquinas a comprender las palabras y frases que usamos para que puedan interactuar con nosotros.\n",
        "Ejemplo simple: Piensa en un asistente virtual como Siri o Google Assistant. Ellos usan NLP para entender lo que dices y responderte, ya sea para decirte el clima o para enviar un mensaje de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwfwco_S28Nh",
        "outputId": "94fe0df0-f899-4b8a-a1fc-4f36c0bea222"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\juanito\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\juanito/nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\juanito\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      6\u001b[0m texto \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHola, ¿cómo estás? Espero que estes bien papasito\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(texto)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\juanito\\anaconda3\\envs\\juanito\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\juanito/nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\juanito\\\\anaconda3\\\\envs\\\\juanito\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\juanito\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "#tokenizacion\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Hola, ¿cómo estás? Espero que estes bien papasito\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uI8Xm7R9ade"
      },
      "source": [
        "Explicacion:\n",
        "\n",
        "word_tokenize lo que hace es separar las palabras de forma individual (tokens)\n",
        "teniendo como respuesta una lista de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p02uZNSO-Byf",
        "outputId": "17bddba2-3aed-452a-c9ec-44a515bf0b27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\juanito\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['clase', 'programación', 'nivel', 'intermedio']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#eliminar palaras conectores y vacias (stopwords)\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "word = ['esta','es','una','clase','de','programación','de','nivel','intermedio']\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "filtered_words = [word for word in word if word not in stop_words]\n",
        "print(filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh7o8YhBAJj2",
        "outputId": "8ff5ad7d-6c4b-4dab-a790-a55b129c208c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compute\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#LEMATIZACION\n",
        "#reduce una palabbra a su forma base\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = 'computing'\n",
        "lemma = lemmatizer.lemmatize(word, pos='v') #post 'v' le decimos utilizar verbos\n",
        "print(lemma)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAhd9hxACAJX",
        "outputId": "2a63c89b-a478-4fda-8321-5fbc63d11919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'estoy': 2, 'aprendiendo': 1, 'el': 1, 'procesamiento': 1, 'de': 1, 'lenguaje': 1, 'natural': 1, 'y': 1, 'aburrido': 1, 'xd': 1})\n"
          ]
        }
      ],
      "source": [
        "#CONTEO DE PALABRAS con suma redundancia\n",
        "from collections import Counter\n",
        "\n",
        "texto = \"Estoy aprendiendo el procesamiento de lenguaje natural y estoy aburrido xd\"\n",
        "tokens = word_tokenize(texto.lower())\n",
        "word_count = Counter(tokens)\n",
        "print(word_count)\n",
        "\n",
        "'''\n",
        "NNP = nombre propio\n",
        "VBZ = verbo\n",
        "JJ = adjetivo\n",
        "NN = sustantivo\n",
        ", = coma\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzBdTSiJDW9I",
        "outputId": "3aeab3db-5c18-47d6-b7e5-7ccc3fc2f777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Jhon', 'NNP'), ('se', 'NN'), ('esta', 'NN'), ('quedando', 'NN'), ('dormido', 'NN'), (',', ','), ('xd', 'NN')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#ETIQUETADO DE PARTES DE LA ORACION - POST TAGGING\n",
        "#Etiquetas gramaticales (verbos, sustantivos,.....)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "texto = \"Jhon se esta quedando dormido, xd\"\n",
        "tokens = word_tokenize(texto)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYuzDNZPF4bw",
        "outputId": "c671263d-53aa-47d5-8fbc-5ae63dd4faee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "The sun was hidden on the horizon, staining the sky of orange and purple tones.The waves of the sea broke gently on the shore, creating a reassuring melody that accompanied the song of the seagulls.On the beach, a family laughed while building sand castles, immortalizing those moments of happiness.The aroma of salt and sea breeze filled the air, remembering the beauty of the simple and the ephemeral.It was a perfect moment, where time seemed to stop, allowing nature and life to melt into a hug of serenity.\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "texto = \"El sol se ocultaba en el horizonte, tiñendo el cielo de tonos anaranjados y púrpuras. Las olas del mar rompían suavemente en la orilla, creando una melodía tranquilizadora que acompañaba el canto de las gaviotas. En la playa, una familia reía mientras construía castillos de arena, inmortalizando esos momentos de felicidad. El aroma a sal y a brisa marina llenaba el aire, recordando la belleza de lo simple y lo efímero. Era un instante perfecto, donde el tiempo parecía detenerse, permitiendo que la naturaleza y la vida se fundieran en un abrazo de serenidad.\"\n",
        "traduccion = translator.translate(texto, src='es', dest='en')\n",
        "print(traduccion.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WSsR0OqHOGQ",
        "outputId": "60213a3a-b1c4-4f16-d4de-8a30afad8880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reseña: Este producto es malo\n",
            "Sentimiento: Negativo\n",
            "\n",
            "Reseña: No vale la pena, es muy caro para lo que ofrece.\n",
            "Sentimiento: Negativo\n",
            "\n",
            "Reseña: Satisfecho con la compra, buena calidad.\n",
            "Sentimiento: Positivo\n",
            "\n",
            "Reseña: El peor servicio al cliente que he recibido.\n",
            "Sentimiento: Negativo\n",
            "\n",
            "Reseña: Un artículo increíble, funciona de maravilla.\n",
            "Sentimiento: Positivo\n",
            "\n",
            "Reseña: Muy mala experiencia, llegó dañado.\n",
            "Sentimiento: Negativo\n",
            "\n",
            "Reseña: Lo recomiendo, muy buen producto.\n",
            "Sentimiento: Positivo\n",
            "\n",
            "Reseña: Nunca volvería a comprar esto, pésima calidad.\n",
            "Sentimiento: Negativo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalar la librería TextBlob si no está instalada\n",
        "!pip install textblob\n",
        "\n",
        "# Importar las librerías necesarias\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#descargas de recursos\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Inicializarlo lematizacion y stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "\n",
        "# Lista de reseñas de productos\n",
        "reseñas = [\n",
        "    \"Este producto es malo\",\n",
        "    \"No vale la pena, es muy caro para lo que ofrece.\",\n",
        "    \"Satisfecho con la compra, buena calidad.\",\n",
        "    \"El peor servicio al cliente que he recibido.\",\n",
        "    \"Un artículo increíble, funciona de maravilla.\",\n",
        "    \"Muy mala experiencia, llegó dañado.\",\n",
        "    \"Lo recomiendo, muy buen producto.\",\n",
        "    \"Nunca volvería a comprar esto, pésima calidad.\"\n",
        "]\n",
        "\n",
        "\n",
        "#Diccionario de palabras clave y sus puntuaciones\n",
        "puntuaciones = {\n",
        "    'bueno':1,\n",
        "    'satisfecho':1,\n",
        "    'recomiendo':1,\n",
        "    'increíble':1,\n",
        "    'muy bueno':1,\n",
        "    'excelente':1,\n",
        "    'buena':1,\n",
        "    'muy buena':1,\n",
        "    'malo':-1,\n",
        "    'peor':-1,\n",
        "    'pésimo':-1,\n",
        "    'pésima':-1,\n",
        "    'muy malo':-1,\n",
        "    'muy pésimo':-1,\n",
        "    'no vale la pena':-1,\n",
        "    'caro':-1,\n",
        "    'muy caro':-1,\n",
        "    'muy barato':1,\n",
        "    'barato':1,\n",
        "    'dañado':-1,\n",
        "    'mala':-1\n",
        "}\n",
        "\n",
        "# Función para realizar análisis de sentimientos con TextBlob\n",
        "def analizar_sentimiento(texto):\n",
        "    #tokenizar y lematizar el texto\n",
        "    palabras = word_tokenize(texto.lower())\n",
        "    palabras_limpias = [lemmatizer.lemmatize(palabra) for palabra in palabras if palabra.isalnum() and palabra not in stop_words]\n",
        "\n",
        "    #Analizar el sentimiento con el textblob\n",
        "    blob = TextBlob(' '.join(palabras_limpias))\n",
        "    polaridad = blob.sentiment.polarity  # Polaridad entre -1 (negativo) y 1 (positivo)\n",
        "\n",
        "    #Ajustar la polaridad segun la palabra clave\n",
        "    score = polaridad\n",
        "    for palabra in palabras_limpias:\n",
        "        if palabra in puntuaciones:\n",
        "            score += puntuaciones[palabra]\n",
        "    #clasificacion del sentimiento\n",
        "    if score > 0.2:\n",
        "        return 'Positivo'\n",
        "    elif score < -0.2:\n",
        "        return 'Negativo'\n",
        "    else:\n",
        "        return 'Neutro'\n",
        "\n",
        "# Aplicar análisis de sentimientos a cada reseña\n",
        "for reseña in reseñas:\n",
        "    sentimiento = analizar_sentimiento(reseña)\n",
        "    print(f\"Reseña: {reseña}\")\n",
        "    print(f\"Sentimiento: {sentimiento}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsSMQz5McM4a",
        "outputId": "33c571e5-fd0c-4d84-db74-982ed5c76d60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens de los Comentarios\n",
            "[['me', 'encantó', 'el', 'producto', ',', 'lo', 'recomiendo', 'para', 'todos', '!'], ['el', 'servicio', 'fue', 'muy', 'malo', ',', 'no', 'volveré'], ['la', 'calidad', 'es', 'excelente', ',', 'estoy', 'muy', 'satisfecho', 'con', 'la', 'compra'], ['es', 'un', 'artículo', 'caro', 'y', 'no', 'vale', 'la', 'pena'], ['el', 'producto', 'llegó', 'dañado', ',', 'estoy', 'muy', 'decepcionado'], ['no', 'me', 'gustó', 'el', 'producto', ',', 'no', 'lo', 'recomiendo'], ['excelente', 'servicio', ',', 'volvería', 'a', 'comprar', 'nuevamente'], ['el', 'producto', 'es', 'de', 'buena', 'calidad', ',', 'lo', 'recomiendo']]\n",
            "Comentarios limpios\n",
            "[['encantó', 'producto', ',', 'recomiendo', '!'], ['servicio', 'malo', ',', 'volveré'], ['calidad', 'excelente', ',', 'satisfecho', 'compra'], ['artículo', 'caro', 'vale', 'pena'], ['producto', 'llegó', 'dañado', ',', 'decepcionado'], ['gustó', 'producto', ',', 'recomiendo'], ['excelente', 'servicio', ',', 'volvería', 'comprar', 'nuevamente'], ['producto', 'buena', 'calidad', ',', 'recomiendo']]\n",
            "Frecuencia de las palabras\n",
            "Counter({',': 7, 'producto': 4, 'recomiendo': 3, 'servicio': 2, 'calidad': 2, 'excelente': 2, 'encantó': 1, '!': 1, 'malo': 1, 'volveré': 1, 'satisfecho': 1, 'compra': 1, 'artículo': 1, 'caro': 1, 'vale': 1, 'pena': 1, 'llegó': 1, 'dañado': 1, 'decepcionado': 1, 'gustó': 1, 'volvería': 1, 'comprar': 1, 'nuevamente': 1, 'buena': 1})\n",
            "\n",
            "Análisis de Sentimientos\n",
            "Conteo de Palabras Positivas: 7\n",
            "Conteo de Palabras Negativas: 3\n",
            "Sentimiento general: positivo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Descargar recursos nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Generar una lista de comentarios en español\n",
        "comentarios = [\n",
        "    \"Me encantó el producto, lo recomiendo para todos!\",\n",
        "    \"El servicio fue muy malo, no volveré\",\n",
        "    \"La calidad es excelente, estoy muy satisfecho con la compra\",\n",
        "    \"Es un artículo caro y no vale la pena\",\n",
        "    \"El producto llegó dañado, estoy muy decepcionado\",\n",
        "    \"No me gustó el producto, no lo recomiendo\",\n",
        "    \"Excelente servicio, volvería a comprar nuevamente\",\n",
        "    \"El producto es de buena calidad, lo recomiendo\"\n",
        "]\n",
        "\n",
        "# Tokenización de los comentarios\n",
        "tokens_comentarios = [word_tokenize(comentario.lower()) for comentario in comentarios]\n",
        "print(\"Tokens de los Comentarios\")\n",
        "print(tokens_comentarios)\n",
        "\n",
        "# Eliminar los StopWords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "comentarios_filtrados = [[word for word in tokens if word not in stop_words] for tokens in tokens_comentarios]\n",
        "print(\"Comentarios limpios\")\n",
        "print(comentarios_filtrados)\n",
        "\n",
        "# Contar la frecuencia de las palabras\n",
        "todas_las_palabras = [palabra for comentario in comentarios_filtrados for palabra in comentario]\n",
        "frecuencia_palabras = Counter(todas_las_palabras)\n",
        "print(\"Frecuencia de las palabras\")\n",
        "print(frecuencia_palabras)\n",
        "\n",
        "# Definir listas de palabras positivas y negativas\n",
        "palabras_positivas = ['bueno', 'satisfecho', 'recomiendo', 'increíble', 'muy bueno', 'excelente', 'buena', 'muy buena']\n",
        "palabras_negativas = ['malo', 'peor', 'pésimo', 'pésima', 'muy malo', 'muy pésimo', 'no vale la pena', 'caro', 'muy caro', 'muy barato', 'barato', 'dañado', 'mala']\n",
        "\n",
        "# Contar las palabras positivas y negativas en los comentarios\n",
        "conteo_positivo = sum(frecuencia_palabras[word] for word in palabras_positivas)\n",
        "conteo_negativo = sum(frecuencia_palabras[word] for word in palabras_negativas)\n",
        "\n",
        "print(\"\\nAnálisis de Sentimientos\")\n",
        "print(f\"Conteo de Palabras Positivas: {conteo_positivo}\")\n",
        "print(f\"Conteo de Palabras Negativas: {conteo_negativo}\")\n",
        "\n",
        "# Determinar el sentimiento general del comentario\n",
        "if conteo_positivo > conteo_negativo:\n",
        "    print(\"Sentimiento general: positivo\")\n",
        "elif conteo_negativo > conteo_positivo:\n",
        "    print(\"Sentimiento general: negativo\")\n",
        "else:\n",
        "    print(\"Sentimiento general: neutral\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W7ffeyPjcT7"
      },
      "source": [
        "LIBRERIAS SCIPY Y NLTK\n",
        "Py ✅\n",
        "LIBRERIAS SCIPY Y NLTK Scipy: desarrollo de funciones matematicas avanzadas NLTK:\n",
        "provesamiento de lenguaje natural NLP\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTmXBrNkkJZz",
        "outputId": "85385138-5df2-4c9e-a034-de18af9b3f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2.  -3.5]\n",
            "(0.33333333333333337, 3.700743415417189e-15)\n"
          ]
        }
      ],
      "source": [
        "#RESOLVER ECUACIONES LINEALES\n",
        "#ALGEBRA LINEAL\n",
        "from scipy import linalg\n",
        "import numpy as np\n",
        "\n",
        "x=np.array([[15,8],[12,6]])\n",
        "y=np.array([2,3])\n",
        "respuesta = linalg.solve(x,y)\n",
        "print(respuesta)\n",
        "\n",
        "#intregacion numerica\n",
        "#para calcular el area bajo una curva, cuando las formulas son complejas\n",
        "\n",
        "from scipy.integrate import quad\n",
        "\n",
        "resultado = quad(lambda x: x**2,0,1)\n",
        "print(resultado)\n",
        "\n",
        "#integracion numerica de la funciona x elevado 2, entre los limites 0 y 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAhltLwQm_L0",
        "outputId": "d21072e2-2b62-459d-9705-9b23b2009bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-2.50000002]\n"
          ]
        }
      ],
      "source": [
        "#OPTIMIZACION CON SCIPY\n",
        "#ENCONTRAR EL VALOR MINIMO O MAXIMO DE UNA FUNCION\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "#DEFINIR LA FUNCION\n",
        "def objective(x):\n",
        "  return x**2 + 5*x + 8\n",
        "\n",
        "result = minimize(objective, 0)\n",
        "print(result.x)\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NALT5skOnwTM",
        "outputId": "0556aa80-676e-4289-facd-87e0d2b0b3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hola', 'amiguito', ',', 'estoy', 'muy', 'contento', 'de', 'que', 'vengas', 'a', 'mi', 'casa', 'amiguito']\n"
          ]
        }
      ],
      "source": [
        "#TOKENIZACION DE ORACIONES\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "texto = \"Hola amiguito, estoy muy contento de que vengas a mi casa amiguito\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "juanito",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
